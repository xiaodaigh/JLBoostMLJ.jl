# MLJJLBoost.jl

The [MLJ.jl](https://github.com/alan-turing-institute/MLJ.jl) interface to [JLBoost.jl](https://github.com/xiaodaigh/JLBoost.jl) a hackable implementation of Gradient Boosting Regression Trees.


## Usage Example

```julia
using RDatasets
iris = dataset("datasets", "iris")
iris[!, :is_setosa] .= iris.Species .== "setosa"

using MLJ, MLJBase, MLJJLBoost
X, y = unpack(iris, x->!(x in [:is_setosa, :Species]), ==(:is_setosa))

using MLJJLBoost:JLBoostClassifier
model = JLBoostClassifier()
```

### Simple Fitting

Fit the model
```julia
mljmodel = fit(model, 1, X, y)
```
Predicting using the model

```julia
predict(model, mljmodel.fitresult, X)
```

Feature Importance for simple fitting
One can obtain the feature importance using the `feature_importance` function

```julia
feature_importance(mljmodel.fitresult.treemodel, X, y)
```

### Using MLJ machines

Put the model and data in a machine

```julia
mljmachine  = machine(model, X, y)
```

Fit model using machine

```julia
fit!(mljmachine)
```

Predict using machine

```julia
predict(mljmachine, X)
```

Feature importance using machine

```julia
feature_importance(fitted_params(mljmachine).fitresult, X, y)
```
